{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "print(t.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified JSON saved to /home/liuzhe/new-files/LoRA-XS/utils/dataset-1024-everypath-9-7_modified.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def add_svg_tags(data):\n",
    "    \"\"\"递归地处理 JSON 数据中的 'response' 字段\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if key == 'response':\n",
    "                # 确保 'response' 字段的值是字符串类型\n",
    "                if isinstance(value, str):\n",
    "                    data[key] = f'<svg viewBox=\"0 0 1024 1024\">{value}</svg>'\n",
    "                else:\n",
    "                    print(f\"Warning: The value of 'response' is not a string: {value}\")\n",
    "            elif isinstance(value, (dict, list)):\n",
    "                add_svg_tags(value)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            add_svg_tags(item)\n",
    "\n",
    "def modify_json_file(file_path):\n",
    "    # 读取 JSON 文件\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # 处理 JSON 数据\n",
    "    add_svg_tags(data)\n",
    "\n",
    "    # 将修改后的数据写入新文件或覆盖原文件\n",
    "    new_file_path = file_path.replace('.json', '_modified.json')\n",
    "    with open(new_file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Modified JSON saved to {new_file_path}\")\n",
    "\n",
    "modify_json_file(\"/home/liuzhe/new-files/LoRA-XS/utils/dataset-1024-everypath-9-7.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if labels is not None:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "                '/home/liuzhe/new-files/AI-ModelScope/Mistral-7B-v0___1',\n",
    "                model_max_length=2048,\n",
    "                padding_side=\"right\",\n",
    "                use_fast=True,\n",
    "            )\n",
    "                # 创建一个新变量，过滤掉 -100\n",
    "            filtered_labels = [label for label in labels.tolist()[0] if label != -100]\n",
    "\n",
    "\n",
    "                # print(\"input_ids\",print(input_ids))\n",
    "            svg_text = tokenizer.decode(filtered_labels).replace('</s>',\"\")\n",
    "            # print(\"label\",\"svg_text\")\n",
    "            # input_id_decode = tokenizer.decode(f_input_ids).replace('</s>',\"\")\n",
    "            # print(\"Input\",input_id_decode)\n",
    "            png_data = cairosvg.svg2png(bytestring=svg_text)\n",
    "            # Create an image from the PNG data\n",
    "            image = Image.open(io.BytesIO(png_data))\n",
    "            # Create a new image with a light gray background\n",
    "            background = Image.new('RGB', image.size, (240, 240, 240))  # Light gray color\n",
    "            background.paste(image, (0, 0), image)\n",
    "            # Save the final image as JPG\n",
    "            background.save('/home/liuzhe/new-files/LoRA-XS/utils/output_image1.jpg', 'JPEG')\n",
    "\n",
    "            # 2. 加载并预处理图像\n",
    "            image_path = '/home/liuzhe/new-files/LoRA-XS/utils/output_image1.jpg'\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            # 3. 计算图像的嵌入\n",
    "            inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                image_embeddings = clip_model.get_image_features(**inputs)\n",
    "\n",
    "            def cosine_similarity(a, b):\n",
    "                if a.device != b.device:\n",
    "                    b = b.to(a.device)  # 将 b 移动到 a 的设备上\n",
    "\n",
    "                a = a / a.norm(dim=-1, keepdim=True)\n",
    "                a = a.reshape(-1, 1024)\n",
    "                b = b / b.norm(dim=-1, keepdim=True)\n",
    "                return (a @ b.T).squeeze().mean()\n",
    "            similarity = cosine_similarity(hidden_states,image_embeddings)\n",
    "            print(\"similarity\",similarity)\n",
    "\n",
    "\n",
    "            # Upcast to float if we need to compute the loss to avoid potential precision issues\n",
    "            logits = logits.float()\n",
    "            # print(\"Logits:\", logits)\n",
    "            # print(\"Labels:\", labels)\n",
    "\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Ensure tensors are on the same device\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits, shift_labels)+(1-similarity)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, MistralForCausalLM\n",
    "\n",
    "        >>> model = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        print(\"Shape of hidden_states:\", hidden_states.shape)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "        #     tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        #         '/home/liuzhe/new-files/AI-ModelScope/Mistral-7B-v0___1',\n",
    "        #         model_max_length=2048,\n",
    "        #         padding_side=\"right\",\n",
    "        #         use_fast=True,\n",
    "        #     )\n",
    "        # #     # 创建一个新变量，过滤掉 -100\n",
    "        #     filtered_labels = [label for label in labels.tolist()[0] if label != -100]\n",
    "        # #     # print(\"input_ids\",print(input_ids))\n",
    "        #     svg_text = tokenizer.decode(filtered_labels).replace('</s>',\"\")\n",
    "        #     png_data = cairosvg.svg2png(bytestring=svg_text)\n",
    "        #     # Create an image from the PNG data\n",
    "        #     image = Image.open(io.BytesIO(png_data))\n",
    "        #     # Create a new image with a light gray background\n",
    "        #     background = Image.new('RGB', image.size, (240, 240, 240))  # Light gray color\n",
    "        #     background.paste(image, (0, 0), image)\n",
    "        #     # Save the final image as JPG\n",
    "        #     background.save('/home/liuzhe/new-files/LoRA-XS/utils/output_image1.jpg', 'JPEG')\n",
    "\n",
    "\n",
    "\n",
    "            # Shift so that tokens < n predict n            # Shift so that tokens < n predict n\n",
    "            # filtered_labels = torch.tensor(filtered_labels, device=self.device)\n",
    "            # labels_embeds = self.model.embed_tokens(filtered_labels)\n",
    "            # # 打印labels的embedding shape，用于调试\n",
    "            # print(\"Labels embedding shape:\", labels_embeds.shape)\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            # print(\"forward:labels size:\", labels.size())  \n",
    "            # print(\"forward:labels content:\", labels)    \n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Ensure tensors are on the same device\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits, shift_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbgpt_hub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
