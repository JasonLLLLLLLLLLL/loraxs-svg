{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "# model_id 模型id\n",
    "# cache_dir 本地缓存目录\n",
    "# ignore_file_pattern 无需下载的文件\n",
    "# #模型下载\n",
    "# from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download('AI-ModelScope/CodeLlama-7b-Instruct-hf')\n",
    "# snapshot_download(model_id=\"ZhipuAI/chatglm3-6b\", cache_dir=\"/home/lz/SSD-GreatWall/models\")\n",
    "# model_path = \"/home/lz/SSD-GreatWall/models/ZhipuAI/chatglm3-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 16:07:35,223 - modelscope - INFO - Model revision not specified, use revision: v1.0.0\n",
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 121kB/s]\n",
      "Downloading: 100%|██████████| 73.0/73.0 [00:00<00:00, 17.0kB/s]\n",
      "Downloading: 100%|██████████| 116/116 [00:00<00:00, 85.3kB/s]\n",
      "Downloading: 100%|█████████▉| 9.26G/9.26G [01:30<00:00, 110MB/s] \n",
      "Downloading: 100%|█████████▉| 4.72G/4.72G [00:47<00:00, 107MB/s] \n",
      "Downloading: 100%|██████████| 23.4k/23.4k [00:00<00:00, 683kB/s]\n",
      "Downloading: 100%|██████████| 1.30k/1.30k [00:00<00:00, 1.04MB/s]\n",
      "Downloading: 100%|██████████| 72.0/72.0 [00:00<00:00, 14.4kB/s]\n",
      "Downloading: 100%|██████████| 1.71M/1.71M [00:00<00:00, 6.21MB/s]\n",
      "Downloading: 100%|██████████| 482k/482k [00:00<00:00, 2.74MB/s]\n",
      "Downloading: 100%|██████████| 966/966 [00:00<00:00, 398kB/s]\n"
     ]
    }
   ],
   "source": [
    "# model_id 模型id\n",
    "# cache_dir 本地缓存目录\n",
    "# ignore_file_pattern 无需下载的文件\n",
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "\n",
    "snapshot_download(model_id=\"AI-ModelScope/Mistral-7B-v0.1\", cache_dir=\"/home/lz/SSD-GreatWall/models\")\n",
    "full_path= \"/home/lz/SSD-GreatWall/models/AI-ModelScope/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting openai-clip\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3f/81/26d701ef9fface424b4ca808a5c5674df645ac46447720a540143d11c41e/openai-clip-1.0.1.tar.gz (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ftfy (from openai-clip)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ab/6e/81d47999aebc1b155f81eca4477a616a70f238a2549848c38983f3c22a82/ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Requirement already satisfied: regex in /home/liuzhe/anaconda3/envs/loraxs/lib/python3.10/site-packages (from openai-clip) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/liuzhe/anaconda3/envs/loraxs/lib/python3.10/site-packages (from openai-clip) (4.66.5)\n",
      "Requirement already satisfied: wcwidth in /home/liuzhe/anaconda3/envs/loraxs/lib/python3.10/site-packages (from ftfy->openai-clip) (0.2.13)\n",
      "Building wheels for collected packages: openai-clip\n",
      "  Building wheel for openai-clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=22598d4659ebfc712db12347edc0a72db0aee081962145ff87c8d3c0828aafa2\n",
      "  Stored in directory: /home/liuzhe/.cache/pip/wheels/09/0b/25/cd73a6d37698127a161f73b8435c122865f69e9029ec586ebe\n",
      "Successfully built openai-clip\n",
      "Installing collected packages: ftfy, openai-clip\n",
      "Successfully installed ftfy-6.3.1 openai-clip-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai-clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "\n",
    "# Load the CLIP model\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = clip_preprocess(Image.open(\"/home/liuzhe/new-files/LoRA-XS/utils/output_image1.jpg\")).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "# Encode the image to get the image features\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.encode_image(image)\n",
    "\n",
    "# Print the dimensions of the image features\n",
    "print(image_features.shape)  # 打印图像特征的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Hidden States Shape: torch.Size([1, 7, 4096])\n"
     ]
    }
   ],
   "source": [
    "   from transformers import AutoTokenizer, AutoModel\n",
    "   import torch\n",
    "\n",
    "   # 模型名称\n",
    "   model_name = \"/home/liuzhe/new-files/AI-ModelScope/Mistral-7B-v0___1\"\n",
    "\n",
    "   # 加载分词器和模型\n",
    "   tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "   model = AutoModel.from_pretrained(model_name)\n",
    "   text = \"This is an example sentence.\"\n",
    "\n",
    "   # 编码文本\n",
    "   svg_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "   # 获取模型输出\n",
    "   with torch.no_grad():\n",
    "       outputs = model(**svg_inputs)\n",
    "\n",
    "   # 获取最后一层的隐藏状态\n",
    "   last_hidden_states = outputs.last_hidden_state\n",
    "   print(\"Last Hidden States Shape:\", last_hidden_states.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "            num_logits_to_keep (`int`, *optional*):\n",
    "                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n",
    "                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n",
    "                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, MistralForCausalLM\n",
    "\n",
    "        >>> model = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        # print(\"Hidden states size:\", hidden_states.shape)\n",
    "        if labels is None and not is_torchdynamo_compiling():\n",
    "            logger.warning_once(\n",
    "                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n",
    "            )\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        # TODO: remove the float() operation in v4.46\n",
    "        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "                '/home/liuzhe/new-files/AI-ModelScope/Mistral-7B-v0___1',\n",
    "                model_max_length=2048,\n",
    "                padding_side=\"right\",\n",
    "                use_fast=True,\n",
    "            )\n",
    "                # 创建一个新变量，过滤掉 -100\n",
    "            filtered_labels = [label for label in labels.tolist()[0] if label != -100]\n",
    "\n",
    "\n",
    "                # print(\"input_ids\",print(input_ids))\n",
    "            svg_text = tokenizer.decode(filtered_labels).replace('</s>',\"\")\n",
    "            # print(\"label\",\"svg_text\")\n",
    "            # input_id_decode = tokenizer.decode(f_input_ids).replace('</s>',\"\")\n",
    "            # print(\"Input\",input_id_decode)\n",
    "            png_data = cairosvg.svg2png(bytestring=svg_text)\n",
    "            # Create an image from the PNG data\n",
    "            image = Image.open(io.BytesIO(png_data))\n",
    "            # Create a new image with a light gray background\n",
    "            background = Image.new('RGB', image.size, (240, 240, 240))  # Light gray color\n",
    "            background.paste(image, (0, 0), image)\n",
    "            # Save the final image as JPG\n",
    "            background.save('/home/liuzhe/new-files/LoRA-XS/utils/output_image1.jpg', 'JPEG')\n",
    "\n",
    "            # 2. 加载并预处理图像\n",
    "            image = clip_preprocess(Image.open(\"/home/liuzhe/new-files/LoRA-XS/utils/output_image1.jpg\")).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "            # Encode the image to get the image features\n",
    "            with torch.no_grad():\n",
    "                image_embeddings = clip_model.encode_image(image)\n",
    "\n",
    "#           获得svg last_hidden_state\n",
    "            svg_inputs = tokenizer(svg_text, return_tensors=\"pt\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**svg_inputs)\n",
    "\n",
    "\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "            def cosine_similarity(a, b):\n",
    "                if a.device != b.device:\n",
    "                    b = b.to(a.device)  # 将 b 移动到 a 的设备上\n",
    "                a = a.reshape(-1,512)\n",
    "                # 使用 PyTorch 内置函数计算余弦相似度\n",
    "                similarity = F.cosine_similarity(a, b, dim=-1)\n",
    "\n",
    "                return similarity.mean()\n",
    "            \n",
    "\n",
    "            # similarity = cosine_similarity(last_hidden_states,image_embeddings)\n",
    "            # print(\"similarity\",similarity)\n",
    "\n",
    "\n",
    "            # Upcast to float if we need to compute the loss to avoid potential precision issues\n",
    "            logits = logits.float()\n",
    "            # print(\"Logits:\", logits)\n",
    "            # print(\"Labels:\", labels)\n",
    "\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Ensure tensors are on the same device\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "            # loss = loss_fct(shift_logits, shift_labels)+(1-similarity)*0.1\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
